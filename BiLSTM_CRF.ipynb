{"cells":[{"cell_type":"markdown","metadata":{"id":"dschCt4G8bfB"},"source":["# Install libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60050,"status":"ok","timestamp":1650803950059,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"},"user_tz":-330},"id":"EPzSAwiAQPq-","outputId":"5073ebd0-705e-4897-ccc0-a1271243203d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-gpu==1.15\n","  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n","\u001b[K     |████████████████████████████████| 411.5 MB 7.3 kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.21.6)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.44.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.14.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 20.4 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.0.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 31.4 MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.6)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=5ecb6e9857cf1cc892c29fd8c566f36b604978ce34db2981606ce295df1364d0\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires tensorboard<2.9,>=2.8, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"]}],"source":["pip install tensorflow-gpu==1.15"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5053,"status":"ok","timestamp":1650803955105,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"},"user_tz":-330},"id":"P2vnUweqQPZI","outputId":"2c76b3b8-5737-4b65-89f2-d286651ab5a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras==2.2.4\n","  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n","\u001b[K     |████████████████████████████████| 312 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.21.6)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.2.4 which is incompatible.\n","tensorflow 2.8.0 requires tensorboard<2.9,>=2.8, but you have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","Successfully installed keras-2.2.4\n"]}],"source":["pip install keras==2.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7528,"status":"ok","timestamp":1650803962629,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"},"user_tz":-330},"id":"LdFcsybcQPHL","outputId":"aed91b8b-03ba-491a-edc6-15307f61e5e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-_7m35abr\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-_7m35abr\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.21.6)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=bc3d17910b1717fd11b97c9a4ae70091b97b9770acea35d0c4da72a374cf2e85\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-56d578x7/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"]}],"source":["pip install git+https://www.github.com/keras-team/keras-contrib.git"]},{"cell_type":"markdown","metadata":{"id":"XsEbmTJsGBca"},"source":["# Data loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3sRXbs_PWDAx"},"outputs":[],"source":["# Maximum length of comment\n","max_len = 100\n","# Dimension of embeding vector\n","embedding_dim = 100\n","# Max feature\n","max_feature = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lKqpgr7MGyV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650803998360,"user_tz":-330,"elapsed":35739,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"006baf61-f8bd-4986-b539-6306f72858e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cp \"/content/drive/My Drive/tamil_data_complete.tsv\" \"./tamil_data_complete.tsv\"\n","!cp \"/content/drive/My Drive/tamil_data_partial_spans.tsv\" \"./tamil_data_partial_spans.tsv\"\n","!cp \"/content/drive/My Drive/test_data_final.tsv\" \"./test_data_final.tsv\"\n","!cp \"/content/drive/My Drive/glove.twitter.27B.100d.txt\" \"./glove.twitter.27B.100d.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Lcz8BrSRC0L"},"outputs":[],"source":["# Read data\n","import pandas as pd\n","from ast import literal_eval\n","\n","data = pd.read_csv(\"tamil_data_complete.tsv\",sep=\"\\t\")\n","dev = pd.read_csv(\"tamil_data_partial_spans.tsv\",sep=\"\\t\")\n","test = pd.read_csv(\"test_data_final.tsv\",sep=\"\\t\")\n","\n","text_data = data['spans'].values\n","spans = data['text'].apply(literal_eval)\n","lbl = [1 if len(s) > 0 else 0 for s in spans]\n","\n","text_data_test = test['text'].values\n","# spans_test = test['text'].apply(literal_eval)\n","test_id = test.index\n","# lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n","\n","text_data_dev = dev['spans'].values\n","spans_dev = dev['text'].apply(literal_eval)\n","dev_id = dev.index\n","lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8O6SVdILRyAh"},"outputs":[],"source":["# Token level \n","\n","# TweetTokenizer used for the tokanizetion\n","\n","from nltk.tokenize import TweetTokenizer\n","import numpy as np\n","import spacy\n","\n","tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    return tknzr2.tokenize(text_data)\n","\n","## Retrive the toxic word from th given text\n","\n","def retrieve_word_from_span(lst_span, text):\n","    i = 0\n","    token = []\n","    a = 0\n","\n","    word = []\n","\n","    while (i < (len(lst_span) - 1)):\n","        if (lst_span[i] != (lst_span[i+1]-1)):\n","            token.append(lst_span[a:(i+1)])\n","            a = i + 1\n","        elif i == (len(lst_span) - 2):\n","            token.append(lst_span[a:i+2])\n","\n","        i = i + 1\n","\n","    for t in token:\n","        word.append(text[t[0]:(t[len(t)-1])+1])\n","\n","    return word\n","\n","\n","\n","def span_retrived(text_data, spans):\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n","    \n","    return token_labels\n","\n","def span_convert(text_data, spans):\n","    MAX_LEN = 0\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n","\n","    lst_seq = []\n","    for i in range(0, len(text_data)):\n","        # token = tknzr.tokenize(text_data[i])\n","        token = custom_tokenizer(text_data[i])\n","        if len(token) > MAX_LEN:\n","            MAX_LEN = len(token)\n","            \n","        seq = np.zeros(len(token), dtype=int)\n","        for j in range(0, len(token)):\n","            for t in token_labels[i]:\n","                # if token[j] in tknzr.tokenize(t):\n","                if token[j] in custom_tokenizer(t):\n","                    seq[j] = 1\n","        lst_seq.append(seq)     \n","\n","    return (token_labels, lst_seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jI5OfwZYPaMO"},"outputs":[],"source":["from copy import deepcopy\n","\n","# convert data\n","data['token'], data['seq'] = span_convert(text_data, spans)\n","dev['token'], dev['seq'] = span_convert(text_data_dev, spans_dev)\n","# test['token'], test['seq'] = span_convert(text_data_test, spans_test)\n","\n","train = deepcopy(data)\n","data = pd.concat([data, dev])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"vJb8f7lQV1-J","executionInfo":{"status":"ok","timestamp":1650804007711,"user_tz":-330,"elapsed":18,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"5c6dde0b-3c49-4fb5-ae93-6f96fcb33860"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  spans  \\\n","0     Correct. Enga apa military da oodi vilaiyada s...   \n","1     Dei Rajini pavam da ne varaven poraven ellam k...   \n","2     Dey dey deyyy,, loosu pasangala,, munna pinna ...   \n","3     Intha maari comments ku like kekuravangala ind...   \n","4     250 k likes ineram sila arivuketta pundaika it...   \n","...                                                 ...   \n","4811  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n","4812  Trailer Nala irukanu oru than comment pandranu...   \n","4813  Wigpathy Visay na Padam Flop than ithula Kabal...   \n","4814  Vikram ella styleum set aaguthu.. Namba moonji...   \n","4815                     8k dislike sure all vijay fans   \n","\n","                                                   text  \\\n","0     [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...   \n","1     [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...   \n","2     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n","3     [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 5...   \n","4     [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   \n","...                                                 ...   \n","4811                   [55, 56, 57, 58, 59, 60, 61, 62]   \n","4812  [83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...   \n","4813  [24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...   \n","4814   [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n","4815  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n","\n","                                                  token  \\\n","0                  [oodi vilaiyada solli tharuvaaaayaa]   \n","1     [varaven poraven ellam karithuppatha koraiya p...   \n","2     [Dey dey deyyy,, loosu pasangala,,  pathruking...   \n","3                    [ india va vittu veliya annupanum]   \n","4                  [arivuketta pundaika ithku karanam ]   \n","...                                                 ...   \n","4811                                           [gaandu]   \n","4812                                  [yarda nengalam.]   \n","4813                       [Flop t, Vekkam kettavangal]   \n","4814                                       [set aagala]   \n","4815                                   [all vijay fans]   \n","\n","                                                    seq  \n","0                        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]  \n","1               [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]  \n","2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, ...  \n","3                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  \n","4     [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n","...                                                 ...  \n","4811                              [0, 0, 0, 0, 0, 0, 1]  \n","4812  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n","4813  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4814            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]  \n","4815                                 [0, 0, 0, 1, 1, 1]  \n","\n","[4816 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-c533b096-b28b-4410-a9bf-e2c3e566e248\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spans</th>\n","      <th>text</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Correct. Enga apa military da oodi vilaiyada s...</td>\n","      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...</td>\n","      <td>[oodi vilaiyada solli tharuvaaaayaa]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Dei Rajini pavam da ne varaven poraven ellam k...</td>\n","      <td>[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...</td>\n","      <td>[varaven poraven ellam karithuppatha koraiya p...</td>\n","      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Dey dey deyyy,, loosu pasangala,, munna pinna ...</td>\n","      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n","      <td>[Dey dey deyyy,, loosu pasangala,,  pathruking...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Intha maari comments ku like kekuravangala ind...</td>\n","      <td>[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 5...</td>\n","      <td>[ india va vittu veliya annupanum]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>250 k likes ineram sila arivuketta pundaika it...</td>\n","      <td>[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n","      <td>[arivuketta pundaika ithku karanam ]</td>\n","      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4811</th>\n","      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n","      <td>[55, 56, 57, 58, 59, 60, 61, 62]</td>\n","      <td>[gaandu]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4812</th>\n","      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n","      <td>[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...</td>\n","      <td>[yarda nengalam.]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4813</th>\n","      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n","      <td>[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...</td>\n","      <td>[Flop t, Vekkam kettavangal]</td>\n","      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4814</th>\n","      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n","      <td>[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n","      <td>[set aagala]</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4815</th>\n","      <td>8k dislike sure all vijay fans</td>\n","      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n","      <td>[all vijay fans]</td>\n","      <td>[0, 0, 0, 1, 1, 1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4816 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c533b096-b28b-4410-a9bf-e2c3e566e248')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c533b096-b28b-4410-a9bf-e2c3e566e248 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c533b096-b28b-4410-a9bf-e2c3e566e248');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["train"]},{"cell_type":"markdown","metadata":{"id":"WaHOdIeNKBqd"},"source":["# Evaluation metric "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"as0mrBcck7eR"},"outputs":[],"source":["# Evaluation metric\n","\n","import sys\n","import os\n","import os.path\n","from scipy.stats import sem\n","import numpy as np\n","from ast import literal_eval\n","\n","def f1(predictions, gold):\n","    \"\"\"\n","    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n","    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n","    :param predictions: a list of predicted offsets\n","    :param gold: a list of offsets serving as the ground truth\n","    :return: a score between 0 and 1\n","    \"\"\"\n","    if len(gold) == 0:\n","        return 1. if len(predictions) == 0 else 0.\n","    if len(predictions) == 0:\n","        return 0.\n","    predictions_set = set(predictions)\n","    gold_set = set(gold)\n","    nom = 2 * len(predictions_set.intersection(gold_set))\n","    denom = len(predictions_set) + len(gold_set)\n","    return float(nom)/float(denom)\n","\n","\n","def evaluate(pred, gold):\n","    \"\"\"\n","    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n","    :param pred: file with predictions\n","    :param gold: file with ground truth\n","    :return:\n","    \"\"\"\n","    # # read the predictions\n","    # pred_lines = pred.readlines()\n","    # # read the ground truth\n","    # gold_lines = gold.readlines()\n","\n","    pred_lines = pred\n","    gold_lines = gold\n","\n","    # only when the same number of lines exists\n","    if (len(pred_lines) == len(gold_lines)):\n","        data_dic = {}\n","        for n, line in enumerate(gold_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n","            else:\n","                raise ValueError('Format problem for gold line %d.', n)\n","\n","        for n, line in enumerate(pred_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                if int(parts[0]) in data_dic:\n","                    try:\n","                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n","                    except ValueError:\n","                        # Invalid predictions are replaced by a default value\n","                        data_dic[int(parts[0])].append([])\n","                else:\n","                    raise ValueError('Invalid text id for pred line %d.', n)\n","            else:\n","                raise ValueError('Format problem for pred line %d.', n)\n","\n","        # lists storing gold and prediction scores\n","        scores = []\n","        for id in data_dic:\n","            if len(data_dic[id]) == 2:\n","                gold_spans = data_dic[id][0]\n","                pred_spans = data_dic[id][1]\n","                scores.append(f1(pred_spans, gold_spans))\n","            else:\n","                sys.exit('Repeated id in test data.')\n","\n","        return (np.mean(scores), sem(scores))"]},{"cell_type":"markdown","metadata":{"id":"G2WZCOctatQs"},"source":["# Data analysis "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05MJABVLa97Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804008490,"user_tz":-330,"elapsed":17,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"cbf16faf-0516-4f85-dd7a-a7ab8d7084b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["4816\n"]}],"source":["# Number of training data\n","print(len(train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3jc4SouVmH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804008491,"user_tz":-330,"elapsed":14,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"a3602f87-a9cc-49aa-acd7-78e56521296c"},"outputs":[{"output_type":"stream","name":"stdout","text":["876\n"]}],"source":["# Number of test data\n","print(len(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9gTn4FLV4rL","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1650804008492,"user_tz":-330,"elapsed":12,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"243c5d5b-8951-4217-86db-879b56a7ae29"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  text\n","0    Evanukachum pondati kolandhainu sentiment irun...\n","1        Adei ennada short film la irunthu suturukinga\n","2    Super dialogue  Oruthar mela visvasam kattrath...\n","3    Epadiya jathi padam. Ponnu padama edungada. In...\n","4    Ponnu mella kaivaikuravan Kai mattum illa uyir...\n","..                                                 ...\n","871  காலா.காபலி.அசுரன்.பாரியேரும்.பெருமள் வந்த அப்ப...\n","872               Kekka bekka short film mathri irukey\n","873  Bayangaram... Trailerey ippadina appa Padam en...\n","874  komali rasini vesam pottu tamil ilichavayangal...\n","875  Oruththar Mela katra viswasaththukku maththava...\n","\n","[876 rows x 1 columns]"],"text/html":["\n","  <div id=\"df-ecc2566f-1913-4ede-9e17-96f5f6843f21\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Evanukachum pondati kolandhainu sentiment irun...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Adei ennada short film la irunthu suturukinga</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Super dialogue  Oruthar mela visvasam kattrath...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Epadiya jathi padam. Ponnu padama edungada. In...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ponnu mella kaivaikuravan Kai mattum illa uyir...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>871</th>\n","      <td>காலா.காபலி.அசுரன்.பாரியேரும்.பெருமள் வந்த அப்ப...</td>\n","    </tr>\n","    <tr>\n","      <th>872</th>\n","      <td>Kekka bekka short film mathri irukey</td>\n","    </tr>\n","    <tr>\n","      <th>873</th>\n","      <td>Bayangaram... Trailerey ippadina appa Padam en...</td>\n","    </tr>\n","    <tr>\n","      <th>874</th>\n","      <td>komali rasini vesam pottu tamil ilichavayangal...</td>\n","    </tr>\n","    <tr>\n","      <th>875</th>\n","      <td>Oruththar Mela katra viswasaththukku maththava...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>876 rows × 1 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecc2566f-1913-4ede-9e17-96f5f6843f21')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ecc2566f-1913-4ede-9e17-96f5f6843f21 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ecc2566f-1913-4ede-9e17-96f5f6843f21');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}],"source":["test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tu3EjmPXbB8A","colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"status":"ok","timestamp":1650804008493,"user_tz":-330,"elapsed":12,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"8f5961d6-fbc4-4644-fa83-c2806ac42c5e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  spans  \\\n","4796  ithu yennai arindhal movie yoda copy....amai k...   \n","4797            Heh!! Vedalam teasar ahh break pannitom   \n","4798  Opening nalla than iruku finishing nenachathan...   \n","4799  ena da idhu views likes lam paths parithabama ...   \n","4800  00:24 oru nadigaiya paarthu kekkura kelvija thala   \n","4801         Enna da bgm viswasam NKP bgm Mari irukuthu   \n","4802  Anga thala mattum enatha puthusa panitaru....b...   \n","4803  VANTHUTHENU SOLLU THIRIMPI VANTHENU 20 VARUCHU...   \n","4804      Namaku thaviyanatha nama than adichi vanganum   \n","4805  Namaku yadhuku indha tava illadha vala...  Mr....   \n","4806       Idhu keka beka keka beka short film oda copy   \n","4807  Movie marana massu thala unbreakable  Padam aw...   \n","4808   Iwanellam oru hero welanginamaditan Tamil cinema   \n","4809  Rajini: nan arasiyaluku varuvathu vuruthi Ravi...   \n","4810  hey hi all Tamil idiot people ungala Mari peop...   \n","4811  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n","4812  Trailer Nala irukanu oru than comment pandranu...   \n","4813  Wigpathy Visay na Padam Flop than ithula Kabal...   \n","4814  Vikram ella styleum set aaguthu.. Namba moonji...   \n","4815                     8k dislike sure all vijay fans   \n","\n","                                                   text  \\\n","4796  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...   \n","4797  [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...   \n","4798  [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...   \n","4799  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...   \n","4800  [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...   \n","4801  [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...   \n","4802  [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2...   \n","4803  [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 4...   \n","4804  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...   \n","4805  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...   \n","4806  [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...   \n","4807           [57, 58, 59, 60, 61, 62, 63, 64, 65, 66]   \n","4808  [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3...   \n","4809  [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 7...   \n","4810  [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2...   \n","4811                   [55, 56, 57, 58, 59, 60, 61, 62]   \n","4812  [83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...   \n","4813  [24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...   \n","4814   [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n","4815  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n","\n","                                                  token  \\\n","4796               [movie yoda copy., amai kunjungala ]   \n","4797                                   [break pannitom]   \n","4798               [finishing nenachathan bayama iruku]   \n","4799                   [ parithabama irukuuu.., aamais]   \n","4800           [oru nadigaiya paarthu kekkura kelvija ]   \n","4801                   [viswasam NKP bgm Mari irukuthu]   \n","4802                        [ enatha puthusa panitaru.]   \n","4803  [20 VARUCHUTHAKU MUNNADI EPIDI PONNENU APIDIYE...   \n","4804                                  [adichi vanganum]   \n","4805                              [tava illadha vala..]   \n","4806                              [short film oda copy]   \n","4807                                       [kolanasam.]   \n","4808                               [welanginamaditan T]   \n","4809                           [yara yamatha pakurika(]   \n","4810  [idiot people , avan inga naata sorandi thinba...   \n","4811                                           [gaandu]   \n","4812                                  [yarda nengalam.]   \n","4813                       [Flop t, Vekkam kettavangal]   \n","4814                                       [set aagala]   \n","4815                                   [all vijay fans]   \n","\n","                                                    seq  \n","4796  [0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n","4797                           [0, 0, 0, 0, 0, 0, 1, 1]  \n","4798                           [0, 0, 0, 1, 1, 1, 1, 1]  \n","4799  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n","4800                              [0, 1, 1, 1, 1, 1, 0]  \n","4801                           [0, 0, 1, 1, 1, 1, 1, 1]  \n","4802                     [0, 0, 0, 1, 1, 1, 0, 0, 0, 0]  \n","4803  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n","4804                                 [0, 0, 0, 0, 1, 1]  \n","4805                  [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]  \n","4806                        [0, 0, 0, 0, 0, 1, 1, 1, 1]  \n","4807         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n","4808                                 [0, 0, 0, 1, 0, 0]  \n","4809  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...  \n","4810  [0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n","4811                              [0, 0, 0, 0, 0, 0, 1]  \n","4812  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n","4813  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4814            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]  \n","4815                                 [0, 0, 0, 1, 1, 1]  "],"text/html":["\n","  <div id=\"df-191fd3f6-1579-48b0-8196-1f57ca3afbbf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>spans</th>\n","      <th>text</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4796</th>\n","      <td>ithu yennai arindhal movie yoda copy....amai k...</td>\n","      <td>[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...</td>\n","      <td>[movie yoda copy., amai kunjungala ]</td>\n","      <td>[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4797</th>\n","      <td>Heh!! Vedalam teasar ahh break pannitom</td>\n","      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...</td>\n","      <td>[break pannitom]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4798</th>\n","      <td>Opening nalla than iruku finishing nenachathan...</td>\n","      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...</td>\n","      <td>[finishing nenachathan bayama iruku]</td>\n","      <td>[0, 0, 0, 1, 1, 1, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4799</th>\n","      <td>ena da idhu views likes lam paths parithabama ...</td>\n","      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...</td>\n","      <td>[ parithabama irukuuu.., aamais]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4800</th>\n","      <td>00:24 oru nadigaiya paarthu kekkura kelvija thala</td>\n","      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...</td>\n","      <td>[oru nadigaiya paarthu kekkura kelvija ]</td>\n","      <td>[0, 1, 1, 1, 1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4801</th>\n","      <td>Enna da bgm viswasam NKP bgm Mari irukuthu</td>\n","      <td>[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...</td>\n","      <td>[viswasam NKP bgm Mari irukuthu]</td>\n","      <td>[0, 0, 1, 1, 1, 1, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4802</th>\n","      <td>Anga thala mattum enatha puthusa panitaru....b...</td>\n","      <td>[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2...</td>\n","      <td>[ enatha puthusa panitaru.]</td>\n","      <td>[0, 0, 0, 1, 1, 1, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4803</th>\n","      <td>VANTHUTHENU SOLLU THIRIMPI VANTHENU 20 VARUCHU...</td>\n","      <td>[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 4...</td>\n","      <td>[20 VARUCHUTHAKU MUNNADI EPIDI PONNENU APIDIYE...</td>\n","      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4804</th>\n","      <td>Namaku thaviyanatha nama than adichi vanganum</td>\n","      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...</td>\n","      <td>[adichi vanganum]</td>\n","      <td>[0, 0, 0, 0, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4805</th>\n","      <td>Namaku yadhuku indha tava illadha vala...  Mr....</td>\n","      <td>[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...</td>\n","      <td>[tava illadha vala..]</td>\n","      <td>[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4806</th>\n","      <td>Idhu keka beka keka beka short film oda copy</td>\n","      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...</td>\n","      <td>[short film oda copy]</td>\n","      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4807</th>\n","      <td>Movie marana massu thala unbreakable  Padam aw...</td>\n","      <td>[57, 58, 59, 60, 61, 62, 63, 64, 65, 66]</td>\n","      <td>[kolanasam.]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4808</th>\n","      <td>Iwanellam oru hero welanginamaditan Tamil cinema</td>\n","      <td>[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3...</td>\n","      <td>[welanginamaditan T]</td>\n","      <td>[0, 0, 0, 1, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4809</th>\n","      <td>Rajini: nan arasiyaluku varuvathu vuruthi Ravi...</td>\n","      <td>[62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 7...</td>\n","      <td>[yara yamatha pakurika(]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4810</th>\n","      <td>hey hi all Tamil idiot people ungala Mari peop...</td>\n","      <td>[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2...</td>\n","      <td>[idiot people , avan inga naata sorandi thinba...</td>\n","      <td>[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4811</th>\n","      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n","      <td>[55, 56, 57, 58, 59, 60, 61, 62]</td>\n","      <td>[gaandu]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4812</th>\n","      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n","      <td>[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...</td>\n","      <td>[yarda nengalam.]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4813</th>\n","      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n","      <td>[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...</td>\n","      <td>[Flop t, Vekkam kettavangal]</td>\n","      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4814</th>\n","      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n","      <td>[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n","      <td>[set aagala]</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>4815</th>\n","      <td>8k dislike sure all vijay fans</td>\n","      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n","      <td>[all vijay fans]</td>\n","      <td>[0, 0, 0, 1, 1, 1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-191fd3f6-1579-48b0-8196-1f57ca3afbbf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-191fd3f6-1579-48b0-8196-1f57ca3afbbf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-191fd3f6-1579-48b0-8196-1f57ca3afbbf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}],"source":["# Show example of training data\n","train.tail(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viHoxDqEauyj"},"outputs":[],"source":["# counting word in spans for train \n","len_span = train['token'].apply(len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sw7qNdR-VppI"},"outputs":[],"source":["# counting word in spans for test \n","# len_span_test = test['token'].apply(len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrGstC0whNKs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804008495,"user_tz":-330,"elapsed":13,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"0781a9f1-ef3a-4a20-a945-63d5209a749a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1     0.787998\n","2     0.167774\n","3     0.034468\n","4     0.005814\n","5     0.002699\n","6     0.000623\n","10    0.000415\n","7     0.000208\n","Name: token, dtype: float64"]},"metadata":{},"execution_count":17}],"source":["# Statistic spans by number of word in span for train\n","# number of toxic words in an input sentence (ranges from 1 to 10)\n","len_span.value_counts(normalize=True, sort=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUR6XwF2VuEz"},"outputs":[],"source":["# Statistic spans by number of word in span for test \n","# len_span_test.value_counts(normalize=True, sort=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPJWuIpegKo1","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1650804009048,"user_tz":-330,"elapsed":563,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"e1621c35-d6a6-49e5-befc-b85fd59be584"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATM0lEQVR4nO3de7SldV3H8feHQbxxMZupJmaGmWzQRstLwyhReaMWGIGVFaClK4IsQUwzaWVoZC7xnkYXvIQViUiGE5Kjy5AupnJAFAHJCYGZEeRgimkpTHz7Yz+ntT3nzMyeYZ69z5nf+7XWXud5nv07v/3de82cz/49l9+TqkKS1K79Jl2AJGmyDAJJapxBIEmNMwgkqXEGgSQ1bv9JF7C7li5dWqtXr550GZK0qFx99dV3VdWy+Z5bdEGwevVqpqamJl2GJC0qSW7d0XPuGpKkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMY1FQTLV6wiSa+P5StWTfptStJuWXRTTNwfd2zbwmEvu6zX17j13ON67V+S9ramRgSSpLkMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXG9BkGSY5LclGRzkrPmeX5VkiuSfCrJZ5I8o896JElz9RYESZYA5wHHAuuAk5Ksm9Xs5cDFVfV44ETgT/qqR5I0vz5HBBuAzVV1c1XdA1wEnDCrTQEHd8uHAF/ssR5J0jz6vB/BocCWofWtwBNntXkl8KEkZwAPBY7usR5J0jwmfbD4JOCCqloBPAP4qyRzakpyWpKpJFPT09NjL1KS9mV9BsE2YOXQ+opu27BTgIsBqurfgAcBS2d3VFXnV9X6qlq/bNmynsqVpDb1GQRXAWuTrElyAIODwRtntbkNeDpAkh9gEAR+5ZekMeotCKpqO3A6sAm4kcHZQdcnOSfJ8V2zlwCnJvk08G7geVVVfdUkSZqr15vXV9XlwOWztp09tHwDcFSfNUiSdm7SB4slSRNmEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxvUaBEmOSXJTks1JztpBm19IckOS65P8TZ/1SJLm2r+vjpMsAc4DfgLYClyVZGNV3TDUZi3wO8BRVfWVJN/VVz2SpPn1OSLYAGyuqpur6h7gIuCEWW1OBc6rqq8AVNWdPdYjSZpHn0FwKLBlaH1rt23Y4cDhSf41yceTHDNfR0lOSzKVZGp6erqnciWpTZM+WLw/sBZ4CnAS8LYkD5vdqKrOr6r1VbV+2bJlYy5RkvZtfQbBNmDl0PqKbtuwrcDGqrq3qr4A/DuDYJAkjUmfQXAVsDbJmiQHACcCG2e1uZTBaIAkSxnsKrq5x5okSbP0FgRVtR04HdgE3AhcXFXXJzknyfFds03Al5PcAFwBvLSqvtxXTZKkuXo7fRSgqi4HLp+17eyh5QJe3D0kSRMw6YPFkqQJMwgkqXEjBUEGnpPk7G59VZIN/ZYmSRqHUUcEfwIcyeBcf4D/YjB9hCRpkRv1YPETq+oJST4F0M0LdECPdUmSxmTUEcG93SRyBZBkGXBfb1VJksZm1CB4C/B3wHcl+UPgX4BX91aVJGlsRto1VFUXJrkaeDoQ4JlVdWOvlUmSxmKkIEjycOBO4N1D2x5QVff2VZgkaTxG3TV0DTDNYFK4z3fLtyS5JskP91WcJKl/owbBh4FnVNXSqvpO4FjgMuA3GJxaKklapEYNgidV1aaZlar6EHBkVX0ceGAvlUmSxmLU6whuT/IyBrebBPhF4EvdKaWeRipJi9ioI4KTGdxY5tLusarbtgT4hX5KkySNw6inj94FnLGDpzfvvXIkSeM26umjy4DfBh4NPGhme1U9rae6JEljMuquoQuBzwFrgN8HbmFwK0pJ0iI3ahB8Z1W9A7i3qq6sql8BHA1I0j5g1LOGZq4gvj3JTwFfBB7eT0mSpHEaNQheleQQ4CXAW4GDgRf1VpUkaWxGDYKvVNXdwN3AUwGSHNVbVZKksRn1GMFbR9wmSVpkdjoiSHIk8CPAsiQvHnrqYAYXk0mSFrld7Ro6ADiwa3fQ0PavAc/qqyhJ0vjsNAiq6krgyiQXVNWtY6pJkjRGox4sfmCS84HVw7/jlcWStPiNGgTvBf4MeDvwv/2VI0kat1GDYHtV/WmvlUiSJmLU00f/PslvJFme5OEzj14rkySNxagjgud2P186tK2A79u75UiSxm3U+xGs6bsQSdJkjLRrKMlDkry8O3OIJGuTHNdvaZKkcRj1GMFfAPcwuMoYYBvwql4qkiSN1ahB8Iiqei3ddNRV9d9AeqtKkjQ2owbBPUkezOAAMUkeAXyrt6okSWMz6llDrwA+CKxMciFwFPC8voqSJI3PqGcNfTjJNcCTGOwSOrOq7uq1MknSWIx61tDPMLi6+ANVdRmwPckz+y1NkjQOox4jeEV3hzIAquqrDHYX7VSSY5LclGRzkrN20u7nklSS9SPWI0naS0YNgvna7eqmNkuA84BjgXXASUnWzdPuIOBM4BMj1iJJ2otGDYKpJG9M8oju8Ubg6l38zgZgc1XdXFX3ABcBJ8zT7g+Ac4Fvjly1JGmvGTUIzmBwQdl7GPxB/ybwgl38zqHAlqH1rd22/5fkCcDKqvrAzjpKclqSqSRT09PTI5YsSRrFLs8a6nbxXFZVT92bL5xkP+CNjHAaalWdD5wPsH79+tqbdUhS63Y5Iqiq/wXuS3LIbva9DVg5tL6i2zbjIOAxwEeT3MLg1NSNHjCWpPEa9YKyrwPXJfkw8I2ZjVX1wp38zlXA2iRrGATAicDJQ797N7B0Zj3JR4HfqqqpkauXJN1vowbB+7rHyKpqe5LTgU3AEuCdVXV9knOAqarauHulSpL6MOqVxe/q5hpaVVU3jdp5VV0OXD5r29k7aPuUUfuVJO09o15Z/NPAtQzmGyLJ45L4jV6S9gGjnj76SgbXBXwVoKquxdtUStI+YdQguHd4ionOfXu7GEnS+I16sPj6JCcDS5KsBV4IfKy/siRJ47I7VxY/msHNaP4GuBt4UV9FSZLGZ1cTxz0IeD7w/cB1wJFVtX0chUmSxmNXI4J3AesZhMCxwOt7r0iSNFa7Okawrqp+ECDJO4BP9l+SJGmcdjUiuHdmwV1CkrRv2tWI4LFJvtYtB3hwtx6gqurgXquTJPVup0FQVUvGVYgkaTJGPX1UkrSPMggkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1rtcgSHJMkpuSbE5y1jzPvzjJDUk+k+QjSQ7rsx5J0ly9BUGSJcB5wLHAOuCkJOtmNfsUsL6qfgi4BHhtX/VIkubX54hgA7C5qm6uqnuAi4AThhtU1RVV9d/d6seBFT3WI0maR59BcCiwZWh9a7dtR04B/mG+J5KclmQqydT09PReLFGStCAOFid5DrAeeN18z1fV+VW1vqrWL1u2bLzFSdI+bv8e+94GrBxaX9Ft+zZJjgZ+F3hyVX2rx3okSfPoc0RwFbA2yZokBwAnAhuHGyR5PPDnwPFVdWePtUiSdqC3IKiq7cDpwCbgRuDiqro+yTlJju+avQ44EHhvkmuTbNxBd4ve8hWrSNLrY/mKVZN+m5IWoT53DVFVlwOXz9p29tDy0X2+/kJyx7YtHPayy3p9jVvPPa7X/iXtmxbEwWJJ0uQYBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQQNWL5iFUl6fSxfsWrSb1PSHtp/0gWof3ds28JhL7us19e49dzjeu1fUn8cEUhS4wwCSWqcQSBJjTMIJKlxvQZBkmOS3JRkc5Kz5nn+gUne0z3/iSSr+6xHkjRXb0GQZAlwHnAssA44Kcm6Wc1OAb5SVd8PvAk4t696NBmeuiotfH2eProB2FxVNwMkuQg4AbhhqM0JwCu75UuAP06Sqqoe69IYTfrU1eUrVnHHti29vfb3HLqS27fe1lv/0jikr7+5SZ4FHFNVv9qt/xLwxKo6fajNZ7s2W7v1/+ja3DWrr9OA07rVRwI39VJ0/5YCd+2yVVv8TObyM5nLz2Su3f1MDquqZfM9sSguKKuq84HzJ13H/ZVkqqrWT7qOhcTPZC4/k7n8TObam59JnweLtwErh9ZXdNvmbZNkf+AQ4Ms91iRJmqXPILgKWJtkTZIDgBOBjbPabASe2y0/C/hHjw9I0nj1tmuoqrYnOR3YBCwB3llV1yc5B5iqqo3AO4C/SrIZ+E8GYbEvW/S7t3rgZzKXn8lcfiZz7bXPpLeDxZKkxcEriyWpcQaBJDXOIOhZkpVJrkhyQ5Lrk5w56ZoWiiRLknwqSb9XnC0SSR6W5JIkn0tyY5IjJ13TpCX5ze7/zWeTvDvJgyZd0yQkeWeSO7trr2a2PTzJh5N8vvv5HXvav0HQv+3AS6pqHfAk4AXzTLXRqjOBGyddxALyR8AHq+pRwGNp/LNJcijwQmB9VT2GwUkn+/oJJTtyAXDMrG1nAR+pqrXAR7r1PWIQ9Kyqbq+qa7rl/2Lwn/vQyVY1eUlWAD8FvH3StSwESQ4BfpzBmXRU1T1V9dXJVrUg7A88uLvO6CHAFydcz0RU1T8xOLNy2AnAu7rldwHP3NP+DYIx6mZXfTzwiclWsiC8Gfht4L5JF7JArAGmgb/odpe9PclDJ13UJFXVNuD1wG3A7cDdVfWhyVa1oHx3Vd3eLd8BfPeedmQQjEmSA4G/BV5UVV+bdD2TlOQ44M6qunrStSwg+wNPAP60qh4PfIP7MdTfF3T7vE9gEJLfCzw0yXMmW9XC1F2Iu8fXAhgEY5DkAQxC4MKqet+k61kAjgKOT3ILcBHwtCR/PdmSJm4rsLWqZkaLlzAIhpYdDXyhqqar6l7gfcCPTLimheRLSZYDdD/v3NOODIKeJQmD/b43VtUbJ13PQlBVv1NVK6pqNYODf/9YVU1/06uqO4AtSR7ZbXo63z5le4tuA56U5CHd/6On0/gB9FmGp+h5LvD+Pe3IIOjfUcAvMfjWe233eMaki9KCdAZwYZLPAI8DXj3heiaqGx1dAlwDXMfg71WTU00keTfwb8Ajk2xNcgrwGuAnknyewejpNXvcv1NMSFLbHBFIUuMMAklqnEEgSY0zCCSpcQaBJDXOINCik6SSvGFo/beSvHIv9X1Bkmftjb528To/380wekXfryXtikGgxehbwM8mWTrpQoZ1E6ON6hTg1Kp6al/1SKMyCLQYbWdwYdFvzn5i9jf6JF/vfj4lyZVJ3p/k5iSvSfLsJJ9Mcl2SRwx1c3SSqST/3s2LNHPvhNcluSrJZ5L82lC//5xkI/NcCZzkpK7/zyY5t9t2NvCjwDuSvG5W++VJ/qm78PCzSX5s5n0keVM3N/9Hkizrtp/a1fTpJH+b5CFDn8Nbknyse7+9j3K0eBkEWqzOA57dTd88qscCzwd+gMHV3odX1QYGU2GfMdRuNbCBwTTZf9bdDOUUBrNfHgEcAZyaZE3X/gnAmVV1+PCLJfle4FzgaQyuFD4iyTOr6hxgCnh2Vb10Vo0nA5uq6nFdvdd22x8KTFXVo4ErgVd0299XVUdU1cz9C04Z6ms5g8A5jvtx1an2fbszlJUWjKr6WpK/ZHDjkv8Z8deumpm2N8l/ADNTGl8HDO+iubiq7gM+n+Rm4FHATwI/NPTN+hBgLXAP8Mmq+sI8r3cE8NGqmu5e80IG9xy4dGc1Au/sJiq8tKpmguA+4D3d8l8zmIAN4DFJXgU8DDgQ2DTU16Xd+7ghyR5PUax9nyMCLWZvZvANeHje/u10/66T7AccMPTct4aW7xtav49v/1I0e96VAgKcUVWP6x5rhubG/8b9ehfDLzS4AcmPA9uAC5L88o6adj8vAE6vqh8Efh8YvpXj8PvN3qpR+x6DQItWVf0ncDHfvjvkFuCHu+XjgQfsQdc/n2S/7rjB9wE3Mfim/evdN3WSHD7CjWM+CTw5ydIkS4CTGOzW2aEkhwFfqqq3MdhlNTMV9X7AzGjkZOBfuuWDgNu7up69O29SmuGuIS12bwBOH1p/G/D+JJ8GPsiefVu/jcEf8YOB51fVN5O8ncGxg2u6KZGn2cWtAavq9iRnAVcw+Eb+gara1VTBTwFemuRe4OvAzIjgG8CGJC9nMO/8L3bbf4/BHe+mu58H7cb7lABnH5UWhSRfr6oDJ12H9k3uGpKkxjkikKTGOSKQpMYZBJLUOINAkhpnEEhS4wwCSWrc/wEJAL1q6O/dXAAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}],"source":["# Distribution histogram plot\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","%matplotlib inline\n","\n","fig, ax = plt.subplots()\n","\n","ax.hist(len_span, density=True, edgecolor='k', rwidth=0.8)  # density=False would make counts\n","\n","plt.ylabel('Percentage')\n","plt.xlabel('Number of span');"]},{"cell_type":"markdown","metadata":{"id":"dq3YWgcPFkB1"},"source":["# Word embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4txBO5zya-5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804054126,"user_tz":-330,"elapsed":45082,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"228bb6ed-4f81-4a4e-e9bc-4a2d2e32bcaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["GloVe data loaded\n"]}],"source":["# Read embedding\n","word_dict = []\n","embeddings_index = {}\n","f = open('glove.twitter.27B.100d.txt')\n","for line in f:\n","    values = line.split(' ')\n","    word = values[0] \n","    word_dict.append(word)\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('GloVe data loaded')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8bsiH8BSGjC"},"outputs":[],"source":["words = word_dict\n","num_words = len(words)\n","\n","# Dictionary word:index pair\n","# word is key and its value is corresponding index\n","word_to_index = {w : i + 2 for i, w in enumerate(words)}\n","word_to_index[\"UNK\"] = 1\n","word_to_index[\"PAD\"] = 0\n","\n","# Dictionary lable:index pair\n","idx2word = {i: w for w, i in word_to_index.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tv0jD76X6qP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804054696,"user_tz":-330,"elapsed":12,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"58d7ab72-511f-4bce-e307-ac0001bf5d82"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1193514"]},"metadata":{},"execution_count":22}],"source":["num_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OBCNzNnas7p"},"outputs":[],"source":["# first create a matrix of zeros, this is our embedding matrix\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","# for each word in out tokenizer lets try to find that work in our w2v model\n","for word, i in word_to_index.items():\n","    if i > max_feature:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # we found the word - add that words vector to the matrix\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # doesn't exist, assign a random vector\n","        embedding_matrix[i] = np.random.randn(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exfoi9XHba3G"},"outputs":[],"source":[" # mapping for token cases\n","case2Idx = {'1': 1, '0': 0}\n","caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n","\n","char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n","for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n","    char2Idx[c] = len(char2Idx)"]},{"cell_type":"markdown","metadata":{"id":"BmJ-NOODFb0Y"},"source":["# Data pre-processing "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpsRfA9QF9mL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","y = data['seq']\n","X = data['spans']\n","\n","y_dev = dev['seq']\n","X_dev = dev['spans']\n","\n","# y_test = test['seq']\n","X_test = test['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOGEJfoJWRdo"},"outputs":[],"source":["#train test\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_dev1, y_train, y_dev1 = train_test_split(X, y, test_size = 0.1)"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from nltk.tokenize import TweetTokenizer\n","from nltk.tokenize import word_tokenize\n","from nltk import WordNetLemmatizer\n","\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.initializers import Constant\n","from nltk.corpus import stopwords\n","import re\n","import numpy as np\n","\n","tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    text_data = text_data.lower()\n","    return tknzr2.tokenize(text_data)\n","\n","def preprocess(text):\n","    text = text.lower()\n","\n","    word_list = nltk.word_tokenize(text)\n","    lemma = WordNetLemmatizer()\n","\n","    for w in word_list:\n","        w = lemma.lemmatize(w)\n","\n","    new_text = \"\"\n","    for w in word_list:\n","        new_text = new_text + \" \" + w\n","\n","    return new_text\n","\n","def encoding(X, y, isTest = True):\n","    sentences = []\n","    \n","    for t in X:\n","        sentences.append(custom_tokenizer(t))\n","\n","    X = []\n","    for s in sentences:\n","        sent = []\n","        for w in s:\n","            try:\n","                w = w.lower()\n","                sent.append(word_to_index[w])\n","            except:\n","                sent.append(word_to_index[\"UNK\"])\n","        X.append(sent)\n","           \n","    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n","\n","    if isTest:\n","        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n","        y = to_categorical(y, num_classes=2)\n","    else:\n","        y = None\n","\n","    return (X,y)\n","\n","\n","def decoding(text_data, encoding_text, prediction):\n","    test = [[idx2word[i] for i in row] for row in encoding_text]\n","\n","    lst_token = []\n","\n","    for t in range(0, len(test)):\n","        yy_pred = []\n","        for i in range(0, len(test[t])):\n","            if prediction[t][i] == 1:\n","                yy_pred.append(test[t][i])\n","        lst_token.append(yy_pred)\n","\n","    lis_idx = []\n","    for i in range(0, len(text_data)):\n","        idx = []\n","        for t in lst_token[i]:\n","            index = text_data[i].find(t)\n","            idx.append(index)\n","            for j in range(1, len(t)):\n","                index = index + 1\n","                idx.append(index)\n","        lis_idx.append(idx)\n","\n","    return lis_idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ePMsEwnzlzKh","executionInfo":{"status":"ok","timestamp":1650804147605,"user_tz":-330,"elapsed":6151,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"1f38e50c-fdaf-4647-f3ab-60e400b4e7a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ajE_t3U-PiF"},"outputs":[],"source":["X1, y1 = encoding(X_train, y_train)\n","X2, y2 = encoding(X_dev1, y_dev1)"]},{"cell_type":"markdown","metadata":{"id":"Xjx3vXr8QsKg"},"source":["# **BiLSTM - CRF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1xZkahZwr0g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804153050,"user_tz":-330,"elapsed":448,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"9abd63d6-45ab-474b-cca8-fecf12cd6078"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2427                       [0, 1, 0, 1, 0, 0, 1, 0, 1, 0]\n","3144                          [0, 1, 1, 0, 0, 1, 1, 1, 1]\n","4616           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n","1522                                [0, 1, 0, 0, 0, 0, 0]\n","2600    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","                              ...                        \n","4108                             [1, 1, 1, 1, 0, 0, 0, 0]\n","3362    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","98      [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...\n","1530                                   [0, 0, 1, 1, 1, 0]\n","4246                    [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n","Name: seq, Length: 6729, dtype: object"]},"metadata":{},"execution_count":30}],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ak647R6LDfTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804156605,"user_tz":-330,"elapsed":3067,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"c318a59a-cbc6-46b3-bd68-5a0efaf07759"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 100)               0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 100, 100)          119351400 \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 100, 100)          0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 100, 200)          160800    \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 100, 100)          20100     \n","_________________________________________________________________\n","crf_1 (CRF)                  (None, 100, 2)            210       \n","=================================================================\n","Total params: 119,532,510\n","Trainable params: 119,532,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# BiLSTM - CRF \n","from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n","from keras.models import Model, Input\n","from keras_contrib.layers import CRF\n","# from tensorflow_addons.layers import crf\n","from keras.utils.vis_utils import plot_model\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","# from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n","\n","input = Input(shape = (max_len,))\n","model = Embedding(input_dim=num_words,\n","                    output_dim=embedding_dim,\n","                    embeddings_initializer=Constant(embedding_matrix),\n","                    input_length=max_len,\n","                    trainable=True)(input)\n","\n","model = Dropout(0.1)(model)\n","model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n","model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n","crf = CRF(2)  \n","out = crf(model)  # output\n","\n","\n","model = Model(input, out)\n","model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=['accuracy'])\n","\n","model.summary()\n","\n","plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqpQWwBxDgCu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804309971,"user_tz":-330,"elapsed":153368,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"ac085007-ff35-4b00-8744-56dd77badc01"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 6729 samples, validate on 748 samples\n","Epoch 1/1\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","6729/6729 [==============================] - 90s 13ms/step - loss: 0.1131 - acc: 0.9567 - val_loss: 0.1065 - val_acc: 0.9551\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f510a72ef90>"]},"metadata":{},"execution_count":32}],"source":["from keras.callbacks import ModelCheckpoint\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","checkpointer = ModelCheckpoint(filepath = 'drive/My Drive/CODE/SemVal/model/model_detection_19.h5',\n","                       verbose = 0,\n","                       mode = 'auto',\n","                       save_best_only = True,\n","                       monitor='val_loss')\n","\n","model.fit(X1, np.array(y1), batch_size=64, epochs=1, validation_data=(X2, y2), callbacks=[checkpointer])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHCmOXC9W6HL"},"outputs":[],"source":["## Pridection On Test Data\n","\n","X3, y3 = encoding(X_test,None,isTest = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zunUkYrjDgt8"},"outputs":[],"source":["y_pred = model.predict(X3)\n","y_pred = np.argmax(y_pred, axis=-1)\n","# y_test_true = np.argmax(y3, -1)\n","raw_y = decoding(X_test, X3, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Za5YrY_xcifb"},"outputs":[],"source":["X4, y4 = encoding(X_dev,y_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqBXQ95Oa4DL"},"outputs":[],"source":["y_pred_dev = model.predict(X4)\n","y_pred_dev = np.argmax(y_pred_dev, axis=-1)\n","raw_dev = decoding(X_dev, X4, y_pred_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0n-KMNKaa4-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650804347745,"user_tz":-330,"elapsed":17,"user":{"displayName":"Mohit Madhukar More 19MIA1005","userId":"00583940432550358120"}},"outputId":"993d5e31-02f9-48b8-9d95-be0fb3caa799"},"outputs":[{"output_type":"stream","name":"stdout","text":["12.870293332462415\n"]}],"source":["f1(raw_dev[0], spans_dev[0])\n","\n","acc = []\n","for i in range(0, len(spans_dev)):\n","    acc.append(f1(raw_dev[i], spans_dev[i]))\n","\n","print(np.mean(acc)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xz-hpcXFyEts"},"outputs":[],"source":["new_sub = pd.DataFrame({'text': text_data_test, 'spans': raw_y})\n","\n","new_sub.to_csv('drive/My Drive/CODE/SemVal/test_demo_detection.csv',sep=\"\\t\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibNrm6Gr216C"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"BiLSTM_CRF.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}